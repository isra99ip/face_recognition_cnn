# üåç **FaceID en el Aula: IA para Identificaci√≥n Facial en Tiempo Real**

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-green.svg)](https://www.python.org/downloads/)
[![TensorFlow Lite](https://img.shields.io/badge/TensorFlow-Lite-yellow.svg)](https://www.tensorflow.org/lite)
[![Android 8+](https://img.shields.io/badge/android-8%2B-blue.svg)](https://developer.android.com/)
[![UAC Cusco](https://img.shields.io/badge/UAC-Cusco-orange.svg)](https://uacusco.edu.pe/)

> Proyecto acad√©mico integrando **Visi√≥n Computacional, Deep Learning y Computaci√≥n M√≥vil**: demuestra c√≥mo un modelo CNN puede ser desplegado eficientemente en Android usando TensorFlow Lite.
> Implementado bajo el marco **CRISP-ML** y optimizado con **TensorFlow Lite**.

---

## üñºÔ∏è Im√°genes de Referencia

<p align="center">
  <img src="docs/img/diagrama_crispml.png" width="350" alt="Flujo CRISP-ML">
  <img src="docs/img/demo_mobile.gif" width="200" alt="Demo Android">
  <img src="docs/img/resultados.png" width="350" alt="Resultados">
</p>

*(Puedes cambiar estas rutas o sustituir por tus propias capturas: diagrama, demo y resultados.)*

---

## üß≠ Descripci√≥n General

**FaceID en el Aula** es un sistema de identificaci√≥n facial dise√±ado para reconocer miembros de un grupo utilizando √∫nicamente la c√°mara de un m√≥vil Android.  
El modelo emplea **MobileNetV2**, ajustado (*fine-tuning*) sobre un dataset personalizado y optimizado mediante **cuantizaci√≥n (float16/INT8)** para un rendimiento m√≥vil sin conexi√≥n.

üí° El objetivo es demostrar todo el ciclo de *Machine Learning Engineering* ‚Äî recolecci√≥n de datos, entrenamiento, optimizaci√≥n y despliegue ‚Äî siguiendo buenas pr√°cticas de ingenier√≠a.

---

## üß† Componentes Principales

| Etapa                | Herramienta/Librer√≠a           | Prop√≥sito                                   |
|----------------------|-------------------------------|---------------------------------------------|
| **Captura de Datos** | OpenCV                        | Recolecci√≥n de im√°genes faciales            |
| **Preprocesamiento** | Haar Cascade / Dlib           | Detecci√≥n y recorte autom√°tico de rostros   |
| **Modelado CNN**     | TensorFlow + Keras            | Entrenamiento/transfer learning             |
| **Optimizaci√≥n**     | TensorFlow Lite               | Conversi√≥n y cuantizaci√≥n para m√≥viles      |
| **App M√≥vil**        | Kotlin + CameraX + TFLite     | Inferencia en tiempo real                   |
| **Visualizaci√≥n**    | GitHub Pages + Chart.js       | Presentaci√≥n interactiva de m√©tricas        |

---

## ‚öôÔ∏è Requisitos del Entorno

```bash
# Crear entorno virtual (Linux/macOS)
python -m venv .venv && source .venv/bin/activate
# En Windows:
.venv\Scripts\activate

pip install --upgrade pip
pip install opencv-python-headless tensorflow==2.16.1 tensorflow-model-optimization \
            scikit-learn matplotlib pandas jupyter onnx onnxruntime tflite-support
```

- Python ‚â• 3.10
- GPU CUDA (opcional para acelerar entrenamiento)
- Android Studio Iguana o superior
- Dataset ‚â• 500 im√°genes totales (m√≠nimo 100 por persona)

---

## üß© Estructura del Proyecto

```
faceid-aula/
‚îú‚îÄ 1_data_collection/        # Captura de im√°genes
‚îú‚îÄ 2_data_prep/              # Preprocesamiento y partici√≥n
‚îú‚îÄ 3_model/                  # Entrenamiento/exportaci√≥n del modelo
‚îú‚îÄ 4_mobile_app_android/     # App Android (CameraX + TFLite)
‚îú‚îÄ models/                   # Modelos .h5 / .tflite optimizados
‚îú‚îÄ data/                     # Datos (train/val/test)
‚îî‚îÄ docs/                     # Infograf√≠a web (GitHub Pages)
```

---

## üîÅ Pipeline de Ejecuci√≥n

```bash
# 1Ô∏è‚É£ Captura de rostros (m√≠nimo 100 im√°genes/persona)
python 1_data_collection/capture_opencv.py --person PersonaA --n 150

# 2Ô∏è‚É£ Recorte, normalizaci√≥n y divisi√≥n del dataset
python 2_data_prep/detect_crop.py
python 2_data_prep/split_dataset.py

# 3Ô∏è‚É£ Entrenamiento, evaluaci√≥n y exportaci√≥n TFLite
python 3_model/train_mobilenetv2.py
python 3_model/eval_report.py
python 3_model/tflite_convert.py
```

Se generan:
- Matriz de confusi√≥n (`models/confusion_matrix.csv`)
- Reporte de clasificaci√≥n (Precision, Recall, F1)
- Modelos finales (.h5 y .tflite)

---

## üìä M√©tricas Clave

| Indicador         | Objetivo          | Descripci√≥n                           |
|-------------------|------------------|---------------------------------------|
| Accuracy (Test)   | ‚â• 90%            | Precisi√≥n general del modelo          |
| F1-Score          | ‚â• 0.90           | Balance entre precisi√≥n y exhaustividad |
| Latencia Android  | ‚â§ 500 ms/frame   | Tiempo promedio de inferencia         |
| Tama√±o modelo     | ‚â§ 20 MB          | Ideal para ejecuci√≥n local            |
| FPS               | ‚â• 10             | Fluidez aceptable en m√≥viles          |

Con MobileNetV2 y un dataset balanceado, se alcanzan accuracies del 94‚Äì97% (5 clases).

---

## üì± Despliegue M√≥vil (Android)

**Caracter√≠sticas:**
- Interfaz ligera con CameraX
- Procesamiento 100% local, sin conexi√≥n
- Etiquetado en pantalla por rostro con probabilidad
- Medici√≥n autom√°tica de latencia por frame
- Registro de logs en SQLite o Firebase Local

**Tecnolog√≠as:**
- Kotlin (nativo)
- TensorFlow Lite Interpreter
- Modelo: `faceid_best_float16.tflite`
- Compatibilidad: Android 8.0 (API 24)+

**Configuraci√≥n:**
1. Abre la carpeta del proyecto Android en Android Studio:
   ```
   4_mobile_app_android/Android/
   ```
2. Sigue las instrucciones en el archivo `README_android_full.md`.

---

## üß¨ Innovaciones T√©cnicas

‚úÖ Cuantizaci√≥n h√≠brida: float16 e INT8 para m√°xima eficiencia  
‚úÖ Data Augmentation inteligente: rotaci√≥n, brillo y simetr√≠a aleatoria  
‚úÖ Explicabilidad (XAI): mapas Grad-CAM para visualizaci√≥n de decisi√≥n  
‚úÖ Integraci√≥n Edge TPU: compatibilidad con Coral USB Accelerator y RPi4  
‚úÖ Seguridad biom√©trica local: embeddings cifrados (AES-256)  
‚úÖ Inferencia h√≠brida: soporte opcional en servidor Flask o API REST

---

## üß† Futuras Mejoras

- üì∏ Detecci√≥n multi-rostro con bounding boxes din√°micos
- üîä Integraci√≥n con VoiceID (reconocimiento de voz)
- üß© Reducci√≥n de sesgo de dataset (normalizaci√≥n de tono/fondo)
- ‚òÅÔ∏è Sincronizaci√≥n en la nube (Firebase + almacenamiento privado)
- üß† Migraci√≥n a Vision Transformers (ViT) o EfficientNet-Lite

---

## üåà Infograf√≠a Interactiva (GitHub Pages)
# FaceID en el Aula: Informe T√©cnico y Flujo Completo

---

## 1. üè´ Comprensi√≥n del Negocio

### üéØ Objetivo
- Identificar integrantes (3‚Äì5 identidades) en tiempo real usando la c√°mara de un dispositivo Android **sin conexi√≥n a internet**.

### üì¶ Alcance
- Reconocimiento de **una persona dominante** por frame, modo acad√©mico/prototipo.
- Quedan fuera: detecci√≥n multi-persona en paralelo, verificaci√≥n 1:1, anti-spoofing, p√∫blico abierto.

---

## 2. ‚öôÔ∏è Requerimientos Funcionales

- Captura de video con CameraX (`15‚Äì30 fps`, **720p**).
- Detecci√≥n de rostro principal (ML Kit Face Detection), obtenci√≥n de bounding box.
- Recorte ROI + resize (`128√ó128`), normalizaci√≥n, clasificaci√≥n con TFLite.
- Etiqueta: nombre + probabilidad (umbral configurable; ‚ÄúDesconocido‚Äù si probabilidad < 0.80).
- Suavizado temporal (EMA o ventana de `5‚Äì8 frames`).
- Reporte de latencia media/P95 y logging (sin almacenar im√°genes).

---

## 3. üìà Criterios de √âxito

- **Accuracy (hold-out):** ‚â• 90% 
- **Macro‚ÄëF1:** ‚â• 0.90 
- **Dispersi√≥n F1:** ‚â§ 15 pts por clase
- **Latencia:** < 500 ms/frame (media, gama media Android)
- **Consumo:** < 1.5% bater√≠a/min; CPU ‚â§ 70% sostenida
- **Estabilidad:** variaciones de luz, poses ¬±45¬∞, accesorios (ca√≠da ‚â§ 5 pts)

---

## 4. ‚ö†Ô∏è Supuestos y Riesgos

**Supuestos:**  
- Cara dominante/centrada, distancia 0.5‚Äì1.2 m, `15‚Äì25 fps` √∫tiles

**Riesgos:**
- Dataset desbalanceado, recortes defectuosos, contraluz, confusi√≥n por rasgos similares

**Mitigaci√≥n:**  
- Ampliar datos dif√≠ciles, revisar recortes, ajustar umbral por clase, calibrar augmentaci√≥n

---

## 5. üì¶ Entregables y Trazabilidad

- scripts/*.py (extracci√≥n, entrenamiento, evaluaci√≥n, conversi√≥n)
- models/SavedModel_*, .tflite, classes.txt
- reports/metrics_test.json, confusion_matrix.png, training_log.csv
- App Android (android/app)
- README.md/documentaci√≥n

---

## 6. üîé Comprensi√≥n de Datos

- **dataset/**: izra/*.jpg, joel/*.jpg, martin/*.jpg
- **Captura:** m√≠nimo 100 im√°genes/persona (ideal 200‚Äì300); variabilidad en pose, luz, fondo, accesorios
- **Formato:** JPG/PNG, nitidez, sin motion blur
- **EDA (Exploratorio):** conteo por clase/desbalance, balance, diversidad luz/pose, deduplicados
- **Ejemplo `matrix`:**

| Clase  | # Im√°genes | % Total | Observaciones      |
|--------|------------|-------- |-------------------|
| izra   | 298        | 33.4 %  | buena variabilidad|
| joel   | 298        | 33.7 %  | faltan exteriores |
| martin | 298        | 32.9 %  | gafas escasas     |

---

## 7. üõ†Ô∏è Preparaci√≥n de Datos

### 7.1 Detecci√≥n y Recorte
- HaarCascade/Dlib, ROI con margen 10‚Äì15%. Resize 128√ó128 RGB, normalizar `[0,1]`.

### 7.2 Aumentaci√≥n de Datos (solo train)
| Transformaci√≥n | Valor     | Nota                         |
|----------------|-----------|------------------------------|
| Rotaci√≥n       | ¬±30¬∞      | Evitar >35¬∞                  |
| Shift          | 0.25      | Ancho/alto                   |
| Brillo         | [0.5,1.5] | Contraste realista           |
| Zoom           | 0.3       | Priorizar centrados          |
| Shear          | 0.2       | Perspectiva leve             |
| Flip horizontal| S√≠        | No flip vertical             |
| Fill           | nearest   | Rellenar bordes              |

**Regla:** Si `val_accuracy` ‚Üì y `train_accuracy` ‚Üë, reduce zoom/shear.

### 7.3 Partici√≥n
- train/val/test = 70/15/15.
- test totalmente separado (`test_faces128/`).

---

## 8. üèóÔ∏è Modelado

### 8.1 Arquitectura Baseline CNN (ejemplo)
```python
model = keras.Sequential([
    layers.Input(shape=(128,128,3)),
    layers.Conv2D(32, ...),
    layers.BatchNormalization(),
    ... # Ver informe detallado o script
    layers.Dense(num_classes, activation='softmax')
])
```
### 8.2 Hiperpar√°metros
- lr=5e-5, batch=32, epochs=40‚Äì60...
- Optimizer: Adam, Loss: categorical_crossentropy
- Callbacks: ModelCheckpoint, EarlyStopping...

### 8.3 Transfer Learning (opcional)
- MobileNetV2, input 160‚Äì192 px, width multiplier 0.75‚Äì1.0; excelente para m√≥vil.

### 8.4 Etiquetas y Consistencia
- Generar `classes.txt`, asegurar orden de carpetas = salida del generador

### 8.5 B√∫squeda de Hiperpar√°metros (ejemplo de grid)
| Par√°metro   | Valores         |
|-------------|----------------|
| LR          | 1e-4, 7e-5, 5e-5|
| Dropout     | 0.35/0.40/0.50 |
| Filtros     | (32,64,128)/(48,96,192) |
| Img Size    | 128 / 160      |
| Aug         | zoom/shear     |

---

## 9. üìä Evaluaci√≥n

### 9.1 M√©tricas (test)
- Accuracy global, Precision/Recall/F1 por clase, matriz de confusi√≥n

### 9.2 Protocolo
- Congelar mejor checkpoint, evaluar en `test_faces128/` sin augmentaci√≥n

### 9.3 Ejemplo de metrics_test.json
```json
{
  "accuracy_test": 0.93,
  "macro_f1": 0.923,
  "f1_per_class": {"izra": 0.94, "joel": 0.91, "martin": 0.92},
  "support": {"izra": 297, "joel": 302, "martin": 296}
}
```

### 9.4 Criterios de aceptaci√≥n
- Accuracy ‚â• 90%, Macro‚ÄëF1 ‚â• 0.90, dispersi√≥n F1 ‚â§ 15 pts
- Si F1 < 80% alguna clase: mejorar datos o pipeline

---

## 10. üö¶ Despliegue

### 10.1 Conversi√≥n a TFLite (Ejemplo)
```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(SAVEDMODEL_PATH)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
with open(TFLITE_PATH, "wb") as f: f.write(tflite_model)
```

### 10.2 Flujo Android (Kotlin)
1. CameraX: captura y preprocesa frame
2. ML Kit: bounding box del rostro principal
3. Procesamiento: resize+normalize a 128√ó128
4. Inferencia: TFLite Interpreter con multi-thread/NNAPI
5. Postpro: etiquetado y umbral ‚ÄúDesconocido‚Äù
6. Suavizado temporal (EMA/ventana)
7. Output: Etiqueta + Probabilidad en pantalla

**Tensor Details:**
- Input: 1√ó128√ó128√ó3 float32 (o int8/float16)
- Output: 1√óN_CLASSES (probabilidades)

### 10.3 Pruebas de despliegue
- Latencia real media/P95, resistencia a condiciones variables
- Robustez con y sin suavizado temporal

### 10.4 Telemetr√≠a (sin im√°genes; opt-in)
- Logging de inferencias ‚ÄúDesconocido‚Äù
- Promedio de top-2 predicciones (sin guardar frames)

---

## 11. üóÇÔ∏è Detecci√≥n de rostros: Consideraciones finales

- Mejorar dataset incrementando representatividad y calidad
- Auditar pipeline regularmente para mitigar sesgos y errores sistem√°ticos

---

### üìå Referencias y Cr√©ditos

- Implementaci√≥n bajo el marco CRISP-ML(Q), MobileNetV2, TensorFlow Lite, CameraX.
- Contacto: *Equipo FaceID en el Aula (UAC, Cusco, 2025)*

---

*Esta infograf√≠a resume el ciclo completo de desarrollo y despliegue de IA biom√©trica acad√©mica en Android, resaltando criterios, riesgos y protocolos para lograr robustez, seguridad y explicabilidad.*


---

## üìö Referencias T√©cnicas

- Sandler, M. et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks.
- [TensorFlow Lite Docs](https://www.tensorflow.org/lite)
- CRISP-ML(Q): Springer (2021)
- [Android CameraX API](https://developer.android.com/training/camerax)

---

## üßë‚Äçüíª Autores

**Proyecto:** FaceID en el Aula  
**Facultad:** Ingenier√≠a de Sistemas ‚Äì UAC, Cusco  
**Versi√≥n:** 2.2 (2025)  
**Licencia:** MIT (uso acad√©mico y educativo)

üßæ *"Una IA responsable no reemplaza la mirada humana; la amplifica para crear conocimiento y seguridad en su entorno."*

---

Este texto est√° listo para **copiar y pegar directamente en tu repositorio GitHub** como `README.md` o en la infograf√≠a interactiva.  
Incluye estilo visual, estructura profesional y secciones innovadoras.
